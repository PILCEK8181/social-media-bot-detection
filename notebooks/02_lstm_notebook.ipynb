{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d00c42",
   "metadata": {},
   "source": [
    "# LSTM -Tweet Interarrival Times\n",
    "\n",
    "This notebook trains an LSTM classifier to detect bot accounts based on the interarrival times between their tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a7371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix, \n",
    "                             roc_curve, auc)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "\n",
    "import ijson\n",
    "import csv\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, roc_curve, auc, \n",
    "                             accuracy_score, precision_score, recall_score, f1_score, roc_auc_score)\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef90c961",
   "metadata": {},
   "source": [
    "## Load Tweet Data from JSON Files\n",
    "\n",
    "Load tweet data from tweet0.json through tweet8.json files, extracting tweets, labels, and splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c01252-9bc6-489f-885e-4b5493cb9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "SCRATCH_DIR = \"./scratch_temp\"\n",
    "os.makedirs(SCRATCH_DIR, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(SCRATCH_DIR, \"02processed_timestamps.csv\")\n",
    "DATA_DIR = '../data/twibot22/'\n",
    "\n",
    "print(f\"Running ijson extractor, output: {OUTPUT_FILE}\")\n",
    "print(\"Loading labels...\")\n",
    "df_labels = pd.read_csv(os.path.join(DATA_DIR, 'label.csv'))\n",
    "\n",
    "# Remove 'u' from user id value\n",
    "valid_users = set(df_labels['id'].astype(str).str.replace('u', '', regex=False))\n",
    "\n",
    "print(f\"number of users: {len(valid_users)}.\")\n",
    "\n",
    "del df_labels\n",
    "gc.collect()\n",
    "\n",
    "# CSV setup \n",
    "with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(['user_id', 'timestamp'])\n",
    "\n",
    "    # Iterate over the tweet_x.json files\n",
    "    for i in range(9):\n",
    "        file_path = os.path.join(DATA_DIR, f'tweet_{i}.json')\n",
    "        print(f\"\\nWorking with {file_path}...\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'rb') as f_in:\n",
    "                parser = ijson.items(f_in, 'item')\n",
    "                \n",
    "                count = 0\n",
    "                saved = 0\n",
    "                \n",
    "                for tweet in parser:\n",
    "                    # convert str id to int\n",
    "                    uid = str(tweet.get('author_id', ''))\n",
    "                    \n",
    "                    # Create rows\n",
    "                    if uid in valid_users:\n",
    "                        created_at = tweet.get('created_at')\n",
    "                        if created_at:\n",
    "                            writer.writerow([uid, created_at])\n",
    "                            saved += 1\n",
    "                    \n",
    "                    count += 1\n",
    "                    if count % 2000000 == 0:\n",
    "                        # Control prints\n",
    "                        print(f\"   -> finished {count} tweets... (saved: {saved})\")\n",
    "                        \n",
    "            print(f\" file {i} Fil=nished. Saved in total: {saved}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\" Filer {file_path} does not exitst.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in file {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\n Done, file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4d8d4-394d-4395-9b82-e03478421b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "CSV_PATH = \"./scratch_temp/processed_timestamps.csv\"\n",
    "LABEL_PATH = '../data/twibot22/label.csv'\n",
    "SPLIT_PATH = '../data/twibot22/split.csv'\n",
    "\n",
    "print(\"Loading prepared CSV...\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, dtype={'user_id': str}) \n",
    "\n",
    "print(f\"Loaded {len(df)} records. Converting...\")\n",
    "\n",
    "# str -> datetime conversion\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# 2. conversion to unix timestamp\n",
    "# .astype('int64') outputs ns, division by 10^9 gives us s\n",
    "df['timestamp'] = df['timestamp'].astype('int64') // 10**9\n",
    "\n",
    "print(\"Sorting values by time..\")\n",
    "df.sort_values(by=['user_id', 'timestamp'], inplace=True)\n",
    "\n",
    "print(\"getting Inter-arrival times...\")\n",
    "\n",
    "df['prev_time'] = df.groupby('user_id')['timestamp'].shift(1)\n",
    "df['iat'] = df['timestamp'] - df['prev_time']\n",
    "\n",
    "# delete 1st tweet in the series / invalid ones\n",
    "df = df.dropna(subset=['iat'])\n",
    "df = df[df['iat'] > 0]\n",
    "\n",
    "# Log normalisation ? todo\n",
    "df['iat_log'] = np.log1p(df['iat'])\n",
    "\n",
    "print(\"Creating sequences for each unique user...\")\n",
    "sequences_map = df.groupby('user_id')['iat_log'].apply(list).to_dict()\n",
    "\n",
    "# Ram cleanup\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Finished, sequences created for {len(sequences_map)} users.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BP venv",
   "language": "python",
   "name": "bot_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
