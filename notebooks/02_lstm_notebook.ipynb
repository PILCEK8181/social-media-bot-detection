{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d00c42",
   "metadata": {},
   "source": [
    "# LSTM -Tweet Interarrival Times\n",
    "\n",
    "This notebook trains an LSTM classifier to detect bot accounts based on the interarrival times between their tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6a7371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix, \n",
    "                             roc_curve, auc)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "\n",
    "import ijson\n",
    "import csv\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, roc_curve, auc, \n",
    "                             accuracy_score, precision_score, recall_score, f1_score, roc_auc_score)\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef90c961",
   "metadata": {},
   "source": [
    "## Load Tweet Data from JSON Files\n",
    "\n",
    "Load tweet data from tweet0.json through tweet8.json files, extracting tweets, labels, and splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c01252-9bc6-489f-885e-4b5493cb9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "SCRATCH_DIR = \"./scratch_temp\"\n",
    "os.makedirs(SCRATCH_DIR, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(SCRATCH_DIR, \"processed_timestamps.csv\")\n",
    "DATA_DIR = '../data/twibot22/'\n",
    "\n",
    "print(f\"Running ijson extractor, output: {OUTPUT_FILE}\")\n",
    "print(\"Loading labels...\")\n",
    "df_labels = pd.read_csv(os.path.join(DATA_DIR, 'label.csv'))\n",
    "\n",
    "# Remove 'u' from user id value\n",
    "valid_users = set(df_labels['id'].astype(str).str.replace('u', '', regex=False))\n",
    "\n",
    "print(f\"number of users: {len(valid_users)}.\")\n",
    "\n",
    "del df_labels\n",
    "gc.collect()\n",
    "\n",
    "# CSV setup \n",
    "with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(['user_id', 'timestamp'])\n",
    "\n",
    "    # Iterate over the tweet_x.json files\n",
    "    for i in range(9):\n",
    "        file_path = os.path.join(DATA_DIR, f'tweet_{i}.json')\n",
    "        print(f\"\\nWorking with {file_path}...\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'rb') as f_in:\n",
    "                parser = ijson.items(f_in, 'item')\n",
    "                \n",
    "                count = 0\n",
    "                saved = 0\n",
    "                \n",
    "                for tweet in parser:\n",
    "                    # convert str id to int\n",
    "                    uid = str(tweet.get('author_id', ''))\n",
    "                    \n",
    "                    # Create rows\n",
    "                    if uid in valid_users:\n",
    "                        created_at = tweet.get('created_at')\n",
    "                        if created_at:\n",
    "                            writer.writerow([uid, created_at])\n",
    "                            saved += 1\n",
    "                    \n",
    "                    count += 1\n",
    "                    if count % 2000000 == 0:\n",
    "                        # Control prints\n",
    "                        print(f\"   -> finished {count} tweets... (saved: {saved})\")\n",
    "                        \n",
    "            print(f\" file {i} Fil=nished. Saved in total: {saved}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\" Filer {file_path} does not exitst.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in file {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\n Done, file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec4d8d4-394d-4395-9b82-e03478421b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prepared CSV...\n",
      "Loaded 88217457 records. Converting...\n",
      "Sorting values by time..\n",
      "getting Inter-arrival times...\n",
      "Creating sequences for each unique user...\n",
      "Finished, sequences created for 921037 users.\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "CSV_PATH = \"./scratch_temp/processed_timestamps.csv\"\n",
    "LABEL_PATH = '../data/twibot22/label.csv'\n",
    "SPLIT_PATH = '../data/twibot22/split.csv'\n",
    "\n",
    "print(\"Loading prepared CSV...\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, dtype={'user_id': str}) \n",
    "\n",
    "print(f\"Loaded {len(df)} records. Converting...\")\n",
    "\n",
    "# str -> datetime conversion\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# 2. conversion to unix timestamp\n",
    "# .astype('int64') outputs ns, division by 10^9 gives us s\n",
    "df['timestamp'] = df['timestamp'].astype('int64') // 10**9\n",
    "\n",
    "print(\"Sorting values by time..\")\n",
    "df.sort_values(by=['user_id', 'timestamp'], inplace=True)\n",
    "\n",
    "print(\"getting Inter-arrival times...\")\n",
    "\n",
    "df['prev_time'] = df.groupby('user_id')['timestamp'].shift(1)\n",
    "df['iat'] = df['timestamp'] - df['prev_time']\n",
    "\n",
    "# delete 1st tweet in the series / invalid ones\n",
    "df = df.dropna(subset=['iat'])\n",
    "df = df[df['iat'] > 0]\n",
    "\n",
    "# Log normalisation ? todo\n",
    "df['iat_log'] = np.log1p(df['iat'])\n",
    "\n",
    "print(\"Creating sequences for each unique user...\")\n",
    "sequences_map = df.groupby('user_id')['iat_log'].apply(list).to_dict()\n",
    "\n",
    "# Ram cleanup\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Finished, sequences created for {len(sequences_map)} users.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50ecd4-f08d-4ced-9281-5aeb7749196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading split and labels...\")\n",
    "\n",
    "df_labels = pd.read_csv(LABEL_PATH)\n",
    "df_labels['id'] = df_labels['id'].astype(str).str.replace('u', '', regex=False)\n",
    "label_map = dict(zip(df_labels['id'], (df_labels['label'] == 'bot').astype(int)))\n",
    "\n",
    "df_split = pd.read_csv(SPLIT_PATH)\n",
    "df_split['id'] = df_split['id'].astype(str).str.replace('u', '', regex=False)\n",
    "split_map = dict(zip(df_split['id'], df_split['split']))\n",
    "\n",
    "# set sequnce len\n",
    "MAX_SEQ_LEN = 20 \n",
    "\n",
    "def create_dataset(split_name):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    # Loop trough users\n",
    "    for uid, seq in sequences_map.items():\n",
    "        # Check if user land in set\n",
    "        if split_map.get(uid) != split_name:\n",
    "            continue\n",
    "            \n",
    "        if uid not in label_map:\n",
    "            continue\n",
    "            \n",
    "        # cut or pad to MAX_SEQ_LEN\n",
    "        # latest values\n",
    "        if len(seq) > MAX_SEQ_LEN:\n",
    "            seq = seq[-MAX_SEQ_LEN:]\n",
    "        \n",
    "        # convert to tnesor\n",
    "        tensor_seq = torch.tensor(seq, dtype=torch.float32)\n",
    "        \n",
    "        X_list.append(tensor_seq)\n",
    "        y_list.append(label_map[uid])\n",
    "        \n",
    "    if not X_list:\n",
    "        print(f\"Warning '{split_name}' is empty!\")\n",
    "        return None, None\n",
    "\n",
    "    # pad with zeros\n",
    "    X_padded = pad_sequence(X_list, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # pad manually if needed\n",
    "    if X_padded.size(1) < MAX_SEQ_LEN:\n",
    "        pad_size = MAX_SEQ_LEN - X_padded.size(1)\n",
    "        zeros = torch.zeros(X_padded.size(0), pad_size)\n",
    "        X_padded = torch.cat([zeros, X_padded], dim=1) # Padding zleva\n",
    "        \n",
    "    y_tensor = torch.tensor(y_list, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    # LSTM input format: (Batch, Seq, Feature) / add 1\n",
    "    X_padded = X_padded.unsqueeze(-1)\n",
    "    \n",
    "    return X_padded, y_tensor\n",
    "\n",
    "print(\"creating tensors...\")\n",
    "X_train, y_train = create_dataset('train')\n",
    "X_val, y_val = create_dataset('val') \n",
    "X_test, y_test = create_dataset('test')\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Test shape:  {X_test.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BP venv",
   "language": "python",
   "name": "bot_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
